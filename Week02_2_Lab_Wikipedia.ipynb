{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science\n",
    "\n",
    "## Week 2 Day 2 Lab: Downloading to Wikipedia\n",
    "\n",
    "Today we will review some changes to the Wikipedia code. These changes will considerably alter what you are able to do with this code. The end result will be a set of two folders, `data` and `dataframes` which you can use for analysis of Wikipedia.\n",
    "\n",
    "The code has now been altered on my end in several ways:\n",
    "\n",
    "- use and report curl from special export to get a complete history of a page.\n",
    "- considerably expanded reporting and commenting.\n",
    "- new arguments available to the script include --count_only\n",
    "\n",
    "There is also now a second script available `xml_to_dataframe.py` which can be used to then process these files and turn them into separate DataFrames. These DataFrames are stored as .feather files and can be loaded with the code below.\n",
    "\n",
    "You should review the `xml_to_dataframe.py` file as all the operations within that file have been covered in class with the exception of TQDM but you can see how that works in practice.\n",
    "\n",
    "You will note that this version does not use recursion to count the files. Instead it more literally looks within year and month. This is sufficient for this work, but with a deeper folder structure and one where the structure is less certain this approach would not be robust. On the other hand, by assuming year and month it allows for some interesting statistics about the year and month to be displayed. In your own work you may now consider whether to approach a task with a more general but often more abstract solution or a more specific but often more fragile solution. You can see in Jon's solution that he used a clever way to simply count all the files using a global and letting the global handle the recursion (`download_and_count_revisions_solution.py`).\n",
    "\n",
    "You should now be able to download a complete history for a single wikipedia page and process that as a DataFrame. Confirm that you can do this with the code yourself. Then discuss among your group:\n",
    "\n",
    "1. Which two (or more) public figures are worth comparing and why.\n",
    "2. Prior to any specific time series analysis, consider your expectations for this exploratory comparison.\n",
    "\n",
    "Draw upon your group's potential expertise in social science to come up with a theoretically informed rationale for a given comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in Changes to a Repository\n",
    "\n",
    "First you will want to merge files from an upstream branch (mine). These instructions will show how to do that from the terminal. You will want to be in the oii-fsds-wikipedia folder when entering these commands. Note especially **Step 3**. If you do this it will overwrite `download_wiki_revisions.py` so consider making a backup.\n",
    "\n",
    "1. **Add the original repository as a remote:**\n",
    "\n",
    "   ```sh\n",
    "   git remote add upstream https://github.com/berniehogan/oii-fsds-wikipedia.git\n",
    "   ```\n",
    "\n",
    "2. **Fetch the changes from the original repository:**\n",
    "\n",
    "   ```sh\n",
    "   git fetch upstream\n",
    "   ```\n",
    "\n",
    "3. **Backup any local changes:**\n",
    "   If you have your own versions of files like `download_wiki_revisions.py`, you should rename the file first to avoid conflicts:\n",
    "\n",
    "   ```sh\n",
    "   mv download_wiki_revisions.py download_wiki_revisions_backup.py\n",
    "   ```\n",
    "\n",
    "4. **Merge upstream changes into your local main branch:**\n",
    "\n",
    "   ```sh\n",
    "   git merge upstream/main\n",
    "   ```\n",
    "\n",
    "5. **Resolve any conflicts and commit the changes:**\n",
    "   You should resolve any conflicts that arise during the merge and then commit the changes:\n",
    "\n",
    "   ```sh\n",
    "   git add .\n",
    "   git commit -m \"Merge changes from upstream\"\n",
    "   ```\n",
    "\n",
    "6. **Push the changes to your GitHub repository:**\n",
    "\n",
    "   ```sh\n",
    "   git push origin main\n",
    "   ```\n",
    "\n",
    "7. **Test your code after merging:**\n",
    "   You should test your code to ensure everything works correctly after the integration.\n",
    "\n",
    "By following these steps, you should be able to integrate the latest changes from my repository while preserving your own custom modifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, you can use the script below if you wish in order to run the commands directly within a Jupyter notebook rather than via that terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading revisions for first article...\n",
      "Downloading complete history of Data_science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading revisions: 35.3MiB [00:01, 23.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1709 revisions. Organizing into directory structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1709/1709 [00:04<00:00, 409.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final revision counts:\n",
      "Found 1709 total revisions for 'Data_science'.\n",
      "\n",
      "Breakdown by year:\n",
      "  2012: 91 revisions\n",
      "  2013: 127 revisions\n",
      "  2014: 73 revisions\n",
      "  2015: 143 revisions\n",
      "  2016: 103 revisions\n",
      "  2017: 135 revisions\n",
      "  2018: 190 revisions\n",
      "  2019: 130 revisions\n",
      "  2020: 168 revisions\n",
      "  2021: 133 revisions\n",
      "  2022: 185 revisions\n",
      "  2023: 110 revisions\n",
      "  2024: 121 revisions\n",
      "\n",
      "Downloading revisions for second article...\n",
      "Downloading complete history of Machine_learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading revisions: 238MiB [00:16, 14.4MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3887 revisions. Organizing into directory structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3887/3887 [00:18<00:00, 214.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final revision counts:\n",
      "Found 3887 total revisions for 'Machine_learning'.\n",
      "\n",
      "Breakdown by year:\n",
      "  2003: 6 revisions\n",
      "  2004: 33 revisions\n",
      "  2005: 103 revisions\n",
      "  2006: 138 revisions\n",
      "  2007: 130 revisions\n",
      "  2008: 71 revisions\n",
      "  2009: 74 revisions\n",
      "  2010: 132 revisions\n",
      "  2011: 129 revisions\n",
      "  2012: 113 revisions\n",
      "  2013: 96 revisions\n",
      "  2014: 152 revisions\n",
      "  2015: 219 revisions\n",
      "  2016: 261 revisions\n",
      "  2017: 263 revisions\n",
      "  2018: 270 revisions\n",
      "  2019: 293 revisions\n",
      "  2020: 297 revisions\n",
      "  2021: 244 revisions\n",
      "  2022: 298 revisions\n",
      "  2023: 328 revisions\n",
      "  2024: 237 revisions\n",
      "\n",
      "Converting revisions to DataFrames...\n",
      "Processing with text length only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Machine_learning: 100%|██████████| 4/4 [00:05<00:00,  1.44s/batch]\n",
      "Processing Data_science:   0%|          | 0/2 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Machine_learning:\n",
      "Total revisions: 3887\n",
      "Date range: 2003-05-25 06:03:17+00:00 to 2024-10-21 15:03:51+00:00\n",
      "Unique contributors: 1098\n",
      "Average text length: 59622.2 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data_science:  50%|█████     | 1/2 [00:01<00:01,  1.33s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for Data_science:\n",
      "Total revisions: 1709\n",
      "Date range: 2012-04-11 17:34:10+00:00 to 2024-09-04 22:32:11+00:00\n",
      "Unique contributors: 466\n",
      "Average text length: 19660.1 characters\n",
      "\n",
      "Summary for Tokamak:\n",
      "Total revisions: 5\n",
      "Date range: 2024-10-19 11:58:26+00:00 to 2024-10-21 09:11:35+00:00\n",
      "Unique contributors: 4\n",
      "Average text length: 114381.8 characters\n",
      "\n",
      "Verifying DataFrame contents...\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1709 entries, 479 to 759\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   revision_id  1709 non-null   object             \n",
      " 1   timestamp    1709 non-null   datetime64[ns, UTC]\n",
      " 2   username     1123 non-null   object             \n",
      " 3   userid       1123 non-null   object             \n",
      " 4   comment      1372 non-null   object             \n",
      " 5   text_length  1709 non-null   int64              \n",
      " 6   year         1709 non-null   object             \n",
      " 7   month        1709 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(1), object(6)\n",
      "memory usage: 120.2+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n",
      "    revision_id                 timestamp        username    userid  \\\n",
      "479  1244076203 2024-09-04 22:32:11+00:00      Arachnidly  47739713   \n",
      "475  1244075613 2024-09-04 22:27:46+00:00      Arachnidly  47739713   \n",
      "474  1243594551 2024-09-02 10:24:08+00:00    Michaelmalak  14994222   \n",
      "477  1243592758 2024-09-02 10:03:39+00:00  Iniyavalsha333  48376643   \n",
      "478  1243591540 2024-09-02 09:49:07+00:00    Michaelmalak  14994222   \n",
      "\n",
      "                                               comment  text_length  year  \\\n",
      "479  removed cuz scientific method linked in intro ...        28671  2024   \n",
      "475  removed unrelated journal \"scientific data\" pa...        28695  2024   \n",
      "474  Undid revision [[Special:Diff/1243592758|12435...        28719  2024   \n",
      "477               I added new info about data science.        29437  2024   \n",
      "478  Undid revision [[Special:Diff/1243589808|12435...        28719  2024   \n",
      "\n",
      "    month  \n",
      "479    09  \n",
      "475    09  \n",
      "474    09  \n",
      "477    09  \n",
      "478    09  \n",
      "\n",
      "Basic statistics:\n",
      "Total number of revisions: 1709\n",
      "Date range: from 2012-04-11 17:34:10+00:00 to 2024-09-04 22:32:11+00:00\n",
      "Number of unique editors: 466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data_science: 100%|██████████| 2/2 [00:02<00:00,  1.18s/batch]\n",
      "Processing Tokamak: 100%|██████████| 1/1 [00:00<00:00, 71.22batch/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define articles we want to download\n",
    "article1 = \"Data_science\"\n",
    "article2 = \"Machine_learning\"\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"DataFrames\", exist_ok=True)\n",
    "\n",
    "# Download revisions for both articles\n",
    "print(\"Downloading revisions for first article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article1}\"')\n",
    "print(\"\\nDownloading revisions for second article...\")\n",
    "os.system(f'python download_wiki_revisions.py \"{article2}\"')\n",
    "\n",
    "# Convert all downloaded revisions to DataFrames\n",
    "print(\"\\nConverting revisions to DataFrames...\")\n",
    "os.system('python xml_to_dataframe.py --data-dir ./data --output-dir ./DataFrames')\n",
    "\n",
    "# Load and verify one of the DataFrames\n",
    "print(\"\\nVerifying DataFrame contents...\")\n",
    "df = pd.read_feather(f\"DataFrames/{article1}.feather\")\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display some basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Total number of revisions: {len(df)}\")\n",
    "print(f\"Date range: from {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"Number of unique editors: {df['username'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"DataFrames/{article2}.feather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
